# -------------------------- Residual MLP Denoiser Settings --------------------------

# Timestep embedding dimension (higher = more expressive noise schedule).
ResidualMLPDenoiser.dim_t = 128

# Width of hidden layers (larger = more capacity for high-D inputs).
ResidualMLPDenoiser.mlp_width = 512

# Number of residual layers (depth of the MLP).
ResidualMLPDenoiser.num_layers = 4

# Use fixed random Fourier features for timestep embedding.
ResidualMLPDenoiser.random_fourier_features = False

# Use learned sinusoidal embeddings (ignored if random_fourier_features=True).
ResidualMLPDenoiser.learned_sinusoidal_cond = True
ResidualMLPDenoiser.learned_sinusoidal_dim = 16  # 32 or 64 

# Activation function ('relu' is fast and works well for design/tabular inputs).
ResidualMLPDenoiser.activation = 'relu'  # alternatives: 'silu', 'gelu'

# Use LayerNorm after residual blocks (helps stability in high dimensions).
ResidualMLPDenoiser.layer_norm = True

# -------------------------- Diffusion Model Construction Settings --------------------------

# Normalization type: 'minmax' scales to [0, 1]; better for varied feature ranges.
construct_diffusion_model.normalizer_type = 'identity'  # alt: 'standard' 'minmax' 'identity'

# Denoiser architecture: 'resmlp' is fast and effective for tabular/high-D data.
construct_diffusion_model.denoiser_network = @ResidualMLPDenoiser # alt: 'transformer' 'unet1d'

# Disable normalization at input/output layers if needed for sensitive data ranges.
construct_diffusion_model.disable_terminal_norm = False

# -------------------------- Conditional Sampling & CFG --------------------------

# Probability of dropping conditioning during training (improves generalization).
ElucidatedDiffusion.cond_drop_prob = 0.15  # typically [0.0–0.2]

# Number of denoising steps during sampling (higher = better quality, slower).
ElucidatedDiffusion.num_sample_steps = 1024  # range: [64–1024]

# -------------------------- Elucidated Diffusion Sampling --------------------------

# Data std for normalization; 0.5 works well for [0, 1]-scaled data.
ElucidatedDiffusion.sigma_data = 1.0  # usually 0.5 or 1.0

# Churn: stochastic noise added during sampling; lower = safer for high-D data.
ElucidatedDiffusion.S_churn = 5  # image default: 80

# Noise scale range for applying churn (min, max).
ElucidatedDiffusion.S_tmin = 0.01
ElucidatedDiffusion.S_tmax = 20  # image default: 50

# Noise multiplier during churn; higher = more diverse samples.
ElucidatedDiffusion.S_noise = 1.005  # image default: 1.003

# -------------------------- Training --------------------------

# Batch size for gradient updates. Large values stabilize training if memory allows.
Trainer.train_batch_size = 512  # try 512–2048 based on GPU capacity

# Smaller batches used for evaluation or sampling (if needed).
Trainer.small_batch_size = 512  # match with train_batch_size or smaller

# Learning rate for optimizer; 1e-3 is strong for MLPs, 3e-4 is safer.
Trainer.train_lr = 3e-4  # more stable for small datasets

# Learning rate schedule: cosine decay helps overfitting on small datasets.
Trainer.lr_scheduler = 'cosine'

# L2 regularization to prevent overfitting; stronger for smaller datasets.
Trainer.weight_decay = 1e-3  # try 5e-4 if overfitting observed

# num_epochs = Total training steps * gradient accumulate / (dataset_size / batch_size).
Trainer.train_num_steps = 4000  # can scale up to 6000 for 50k samples

# Save checkpoints + samples every N steps.
Trainer.save_and_sample_every = 500
